{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4.1 - NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language, language used for everyday communication\n",
    "Natural Language Processing computational manipulation of natural language in any from\n",
    "We covered already some aspects in the previous lectures\n",
    "Language technologies prevalent in everyday life\n",
    "But also in the Humanities, we often deal with texts, extracting and organize information from texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NLTK: Basic text analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we demonstrate the power of this module by inspecting some of the prepared corpora that NLTK provides. Later we show how you can build your own corpus, and unleash all the nice tools on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the examples below are taken from the [NLTK book](http://www.nltk.org/book/) Before we start, we should install all the required material. Run the cell below to install the tools and corpora. This can take a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Digital Humanities we often treat text as *raw data*, is input for our programs. Interpretation arise from abstraction, for example the counting of word frequencies. This is a radically different approach than the close reading of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what corpora NLTK provides us with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from nltk.book import *` says as much as \"from NLTK's book module, load all items.\"\n",
    "\n",
    "Nice! We can see includes the script for 'Monty Python and the Holy Grail. But if we want to print text6 we do not get the actual content yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a standard procedure we should track the data type of these objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(text6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we learned previously we might have expected these books to be string. Instead we see a totally new type the NLTK 'Text'. As we see below, converting a your text to this data type has many benefits, it facilitates the 'distant reading' of your corpus. The explain in how, we discuss some populat function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A oft-used technique for distant reading is Keyword In Context Analysis in which we center a whole corpus on specific words of interest. NLTK comes with a `concordance()` method that allows you to do just this. For example, how is the word 'grail' used in the Monty Python and the Holy Grail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text6.concordance('grail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more realistic reserch question would be: how have American presidents used about 'democracy' in their Inaugural adresses? Try to do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.concordance('democracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the word 'monstrous' in Moby Dick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1.concordance('monstrous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B]A concordance permits us to see words in context. For example, we saw that monstrous occurred in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size . What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")\n",
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B]Observe that we get different results for different texts. Austen uses this word quite differently from Melville; for her, monstrous has positive connotations, and sometimes functions as an intensifier like the word very."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text5.similar(\"cool\"))\n",
    "print()\n",
    "print(text1.similar(\"cool\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B]The term common_contexts allows us to examine just the contexts that are shared by two or more words, such as monstrous and very. We have to enclose these words by square brackets as well as parentheses, and separate them with a comma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B]It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text. In 1.2 we see some striking patterns of word usage over the last 220 years (in an artificial text constructed by joining the texts of the Inaugural Address Corpus end-to-end). You can produce this plot as shown below. You might like to try more words (e.g., liberty, constitution), and different texts. Can you predict the dispersion of a word before you view it? As before, take care to get the quotes, commas, brackets and parentheses exactly right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we did not access the actual text. NLTK represent these texts as a list (an in-built data type we encountered earlier) Let's find out where this information is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overview quite some stuff associated with the NLTK Text object. We come back to this later, but with respect to the previous question--where is the text hidden--maybe tokens seems a good option. What type does this attribute belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(text1.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text1.tokens` returns a list, something which are fimiliar with now. So let's print the first hundred tokens of Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text1.tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tokens does Moby Dick comprise? [B] Let's determine the length of a text from start to finish, in terms of the words and punctuation symbols that appear--if you have a closer look at the output of the previous print statement, you'll see that it comprises punctuation marks as individual items. We use the function len to obtain the length of a list, which we'll apply here to the book of Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(text1.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B]A token is the technical name for a sequence of characters — such as hairy, his, or :) — that we want to treat as a group. When we count the number of tokens in a text, say, the phrase to be or not to be, we are counting occurrences of these sequences. Thus, in our example phrase there are two occurrences of to, two of be, and one each of or and not. But there are only four distinct vocabulary items in this phrase. How many distinct words does the book of Genesis contain? To work this out in Python, we have to pose the question slightly differently. The vocabulary of a text is just the set of tokens that it uses, since in a set, all duplicates are collapsed together. In Python we can obtain the vocabulary items of text3 with the command: set(text3). When you do this, many screens of words will fly past. Now try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(text1.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B]Although it has 44,764 tokens, this book has only 2,789 distinct words, or \"word types.\" A word type is the form or spelling of the word independently of its specific occurrences in a text — that is, the word considered as a unique item of vocabulary. Our count of 2,789 items will include punctuation symbols, so we will generally call these unique items types instead of word types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(text1.tokens)) / len(text1.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B] Next, let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(100 * text1.count('whale') / len(text1))\n",
    "print(100 * text3.count('whale') / len(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[B] You may want to repeat such calculations on several texts, but it is tedious to keep retyping the formula. Instead, you can come up with your own name for a task, like \"lexical_diversity\" or \"percentage\", and associate it with a block of code. Now you only have to type a short name instead of one or more complete lines of Python code, and you can re-use it as often as you like. The block of code that does a task for us is called a function, and we define a short name for our function with the keyword def. The next example shows how to define two new functions, lexical_diversity() and  percentage():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def lexical_diversity(text):\n",
    "        return len(set(text)) / len(text)\n",
    "    \n",
    "def percentage(count, total):\n",
    "        return 100 * count / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Which text has the highest lexical diversity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you might wonder: what if I want to investigate *other* texts? Of course, this is possible, but requires some *preprocessing* steps. To transform a document on your disk or on the Web (whcih is just a sequence of characters) to a NLTK `Text` object.\n",
    "\n",
    "In the previous lecture we have already covered a few common preprocessing steps such a removing punctuation and lowercasing. Here we will take a slightly different route, because NLTK takes cares of many of issues that required these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aa alluded to earlier, 'tokens' are the minimal units for the machine to process. We often simply equated this with words--which, in turn where defined as everything between to whitespaces--but the relation is more complex. Luckily, NLTK comes with many read-made tools for splitting strings into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"On the 12th of August, 18-- (just three days after my tenth birthday, when I had been given such wonderful presents), I was awakened at seveno’clock in the morning by Karl Ivanitch slapping the wall close to my head with a fly-flap made of sugar paper and a stick.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'the', '12th', 'of', 'August', ',', '18', '--', '(', 'just', 'three', 'days', 'after', 'my', 'tenth', 'birthday', ',', 'when', 'I', 'had', 'been', 'given', 'such', 'wonderful', 'presents', ')', ',', 'I', 'was', 'awakened', 'at', 'seveno', '’', 'clock', 'in', 'the', 'morning', 'by', 'Karl', 'Ivanitch', 'slapping', 'the', 'wall', 'close', 'to', 'my', 'head', 'with', 'a', 'fly-flap', 'made', 'of', 'sugar', 'paper', 'and', 'a', 'stick', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'the', '12th', 'of', 'August,', '18--', '(just', 'three', 'days', 'after', 'my', 'tenth', 'birthday,', 'when', 'I', 'had', 'been', 'given', 'such', 'wonderful', 'presents),', 'I', 'was', 'awakened', 'at', 'seveno’clock', 'in', 'the', 'morning', 'by', 'Karl', 'Ivanitch', 'slapping', 'the', 'wall', 'close', 'to', 'my', 'head', 'with', 'a', 'fly-flap', 'made', 'of', 'sugar', 'paper', 'and', 'a', 'stick.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'the', '12th', 'of', 'August', '18', 'just', 'three', 'days', 'after', 'my', 'tenth', 'birthday', 'when', 'I', 'had', 'been', 'given', 'such', 'wonderful', 'presents', 'I', 'was', 'awakened', 'at', 'seveno', 'clock', 'in', 'the', 'morning', 'by', 'Karl', 'Ivanitch', 'slapping', 'the', 'wall', 'close', 'to', 'my', 'head', 'with', 'a', 'fly', 'flap', 'made', 'of', 'sugar', 'paper', 'and', 'a', 'stick']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(sentence, pattern='\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'the', '12th', 'of', 'August', ',', '18', '--', '(', 'just', 'three', 'days', 'after', 'my', 'tenth', 'birthday', ',', 'when', 'I', 'had', 'been', 'given', 'such', 'wonderful', 'presents', '),', 'I', 'was', 'awakened', 'at', 'seveno', '’', 'clock', 'in', 'the', 'morning', 'by', 'Karl', 'Ivanitch', 'slapping', 'the', 'wall', 'close', 'to', 'my', 'head', 'with', 'a', 'fly', '-', 'flap', 'made', 'of', 'sugar', 'paper', 'and', 'a', 'stick', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming, in its literal sens, amouns to cutting down the branches of a tree to its stem. But also tokens can be reduced to their stem. Stemming is a crude, rule-based process by which we want group together different variations of a token. [B2] For example, the word eat will have variations like eating, eaten, eats, and so on. In some applications, as it does not make sense to differentiate between eat and eaten, we typically use stemming to club both grammatical variances to the root of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "pst = PorterStemmer()\n",
    "print(pst.stem('loving'))\n",
    "print(pst.stem('loved'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[B2]We are creating different stemmer objects, and applying a stem() method on the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'the', '12th', 'of', 'august', ',', '18', '--', '(', 'just', 'three', 'day', 'after', 'my', 'tenth', 'birthday', ',', 'when', 'I', 'had', 'been', 'given', 'such', 'wonder', 'present', ')', ',', 'I', 'wa', 'awaken', 'at', 'seveno', '’', 'clock', 'in', 'the', 'morn', 'by', 'karl', 'ivanitch', 'slap', 'the', 'wall', 'close', 'to', 'my', 'head', 'with', 'a', 'fly-flap', 'made', 'of', 'sugar', 'paper', 'and', 'a', 'stick', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "stems = [pst.stem(w) for w in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more methodical way of converting all the grammatical/inflected forms of the root of the word. Lemmatization uses context and part of speech to determine the inflected form of the word and applies different normalization rules for each part of speech to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wlem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wa'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlem.lemmatize(\"was\")\n",
    "#print(pst.stem(\"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### From File to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "with open('data/Tolstoy-Childhood.txt','r') as doc:\n",
    "    tokens = word_tokenize(doc.read())\n",
    "    \n",
    "text = nltk.text.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 13 of 13 matches:\n",
      " . Karl Ivanitch sneezed , wiped his nose , flicked his fingers , and began am\n",
      "his call . Karl , with spectacles on nose and a book in his hand , was sitting\n",
      " had slipped down his large aquiline nose , and the blue , half-closed eyes an\n",
      "ied as I caressed her and kissed her nose , “ we are going away today . Good-b\n",
      "mall and perpetually twinkling , his nose large and aquiline , his lips irregu\n",
      "eginning to stand out on my brow and nose . My ears were burning , I trembled \n",
      "hat no human being with such a large nose , such thick lips , and such small g\n",
      " , a turned-up , strongly pronounced nose , very bright red lips ( which , nev\n",
      "iously , as well as of twitching his nose and eyebrows . Consequently every on\n",
      "from mine , she scratched her little nose with her glove ! All this I can see \n",
      " was left visible but the tip of her nose . Indeed , I could see that , if her\n",
      "ss her fingers and eyes and lips and nose and feet -- kiss all of her. ” “ How\n",
      "d , Natalia Savishna , spectacles on nose and engaged in darning stockings . S\n"
     ]
    }
   ],
   "source": [
    "text.concordance('nose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: **to do**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word and rare word removal\n",
    "\n",
    "**to do**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Topics\n",
    "- Part-of-Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Information Extraction: NER and Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**to do**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
