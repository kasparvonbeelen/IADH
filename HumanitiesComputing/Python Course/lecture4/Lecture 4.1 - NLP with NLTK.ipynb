{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4.1 - NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the examples below are taken from the [NLTK book](http://www.nltk.org/book/) Before we start, we should install all the required material. Run the cell below to install the tools and corpora. This can take a minute..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below to install the additional material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python's Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I demonstrate the power of the NLTK by inspecting some of the **prepared corpora** of this library. Later on, I show how you can build your own corpus, and unleash all the nice tools on **your own data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Digital Humanities, we often treat texts as *raw data*, as input for our programs. Interpretations arise from abstraction, for example, by counting word frequencies, analysing specific segments of a corpus (i.e. Key Word In Context, or KWIC analysis) or searching for patterns (i.e. collocations). \n",
    "\n",
    "NLTK provides several tools for both **processing** data and **interpreting** texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what corpora NLTK provides by loading the `book` module from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from nltk.book import *` says as much as \"from NLTK's book module, load all items.\" This loads all the books that are processed in advance for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we discover that NLTK includes the script of 'Monty Python and the Holy Grail'. When we `print` text6 we (surprisingly) can not see the actual content yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a standard procedure, we should uncover the data type of the object we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(text6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To text is stored in the `tokens` attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first hundred tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text6.tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the text is already properly tokenized (i.e. words and punctuation marks are properly separated from each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do more with this `Text` object. To view all the methods attached to this object, use Python's help function. You can ignore all those that start with a double underscore and scroll down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.text.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the methods attached to the `Text` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.concordance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An oft-used technique for distant reading is **Keyword In Context Analysis** in which we centre a whole corpus on a specific word of interest. NLTK comes with a `concordance()` method that allows you to do just this. For example, how is the word 'grail' used in  'Monty Python and the Holy Grail'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(nltk.text.Text.concordance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text6.concordance('grail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more realistic research question would be: how have American presidents used 'democracy' in their Inaugural Addresses since 1861? Try to do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.concordance('democracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the number of hits to print with the `lines` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.concordance('democracy',lines=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about 'monstrous' in Moby Dick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1.concordance('monstrous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.similar()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concordance()` shows words in their context. For example, we saw that monstrous occurred in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size. What other words appear in a **similar range of contexts**? We can find out by appending the term similar to the name of the text in question, then enter the word you want analyse within parentheses (don't forget to put a string between quotation marks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text1: Moby Dick by Herman Melville 1851\n",
    "text1.similar(\"monstrous\")\n",
    "print('\\n')\n",
    "#text2: Sense and Sensibility by Jane Austen 1811\n",
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we get **different results for different texts**. Austen uses this word quite differently from Melville; for her, monstrous has positive connotations and sometimes functions as an intensifier like the word very."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text5.similar(\"cool\"))\n",
    "print('\\n')\n",
    "print(text1.similar(\"cool\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `common_contexts` allows us to **examine just the contexts** that are shared by two or more words, such as monstrous and very. We have to enclose these words by square brackets as well as parentheses, and separate them with a comma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.dispersion_plot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also determine the **location** of a word in the text: how many words from the beginning it appears. This **positional information** can be displayed using a dispersion plot. Each **stripe** represents an instance of a word, and each **row** represents the entire text. In 1.2 we see some striking patterns of word usage over the last 220 years (in an artificial text constructed by joining the texts of the Inaugural Address Corpus end-to-end). You can produce this plot as shown below. You might like to try more words (e.g., liberty, constitution), and different texts. Can you predict the dispersion of a word before you view it? As before, take care to get the quotes, commas, brackets and parentheses exactly right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `collocations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collocation is a **sequence of words that occur together unusually often**. Thus *red wine* is a collocation, whereas *the wine* is not. A characteristic of collocations is that they are resistant to substitution with words that have similar senses; for example, maroon wine sounds definitely odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we still have not inspected the actual text. NLTK represents texts as a list (an inbuilt data type we encountered earlier). Let's find out where this information is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(text1.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text1.tokens` returns a list, something which we are familiar with by now. So let's print the first hundred tokens of Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(text1.tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the length of a text from start to finish, in terms of the **words and punctuation** symbols that appear--if you have a closer look at the output of the previous print statement, you'll see that it comprises punctuation marks as individual items. We use the function len to obtain the length of a list, which we'll apply here to the book of Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(text1.tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Is \"Sense and Sensibility\" longer than \"Moby Dick\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text2.tokens) > len(text1.tokens)\n",
    "# actually this also works: len(text2) > len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **token** is the technical name for a **sequence of characters** — such as hairy, his, or :) — that we want to **treat as a group** (or **unit** of our analysis). When we count the number of tokens in a text, say, the phrase \"to be or not to be\", we are counting occurrences of these sequences. Thus, in our example phrase there are two occurrences of \"to\", two of \"be\", and one each of \"or\" and \"not\". \n",
    "\n",
    "But there are only **four distinct** vocabulary items in this phrase. How many distinct words does the book of Genesis contain? To work this out in Python, we have to pose the question slightly differently. The vocabulary of a text is just the **set of tokens** that it uses, since in a set, **all duplicates are collapsed together**. In Python we can obtain the vocabulary items of text3 with the command: `set(text3)`. When you do this, many screens of words will fly past. Now try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text3.tokens))\n",
    "print(len(set(text3.tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it has 44,764 tokens, this book has only 2,789 distinct words, or \"word types.\" A word type is the form or spelling of the word independently of its specific occurrences in a text — that is, the word considered as a unique item of vocabulary. Our count of 2,789 items will include punctuation symbols, so we will generally call these unique items types instead of word types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: does Moby Dick contain more types than Genesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(text1)) > len(set(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate a measure of the **lexical richness** of the text. The next example shows us that the number of **distinct words** is just 6% of the total number of words, or equivalently that each word is used 16 times on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(text3.tokens)) / len(text3.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on particular words. We can count **how often a word occurs** in a text, and compute what percentage of the text is taken up by a specific word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(100 * text1.count('whale') / len(text1))\n",
    "print(100 * text3.count('whale') / len(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to repeat such calculations on several texts, but it is tedious to keep retyping the formula. Instead, you can come up with your own name for a task, like \"lexical_diversity\" or \"percentage\", and associate it with a block of code. Now you only have to type a short name instead of one or more complete lines of Python code, and you can **re-use** it as often as you like. The block of code that does a task for us is called a **function**, and we define a **short name** for our function with the **keyword def**. The next example shows how to define two new functions, lexical_diversity() and  percentage():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "        return len(set(text)) / len(text)\n",
    "    \n",
    "def percentage(count, total):\n",
    "        return 100 * count / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexical_diversity(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: print the percentage of the word 'poor' in the Genesis using the percentage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = text3.count('poor')\n",
    "total = len(text3)\n",
    "print(percentage(count,total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could make the `percentage` function a bit more useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(word, text):\n",
    "    count = text.count(word)\n",
    "    total = len(text)\n",
    "    return 100 * count / total\n",
    "\n",
    "print(percentage('poor',text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which texts talk most often about 'love'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_count = 'love'\n",
    "\n",
    "maximum = text1.count(word_to_count)\n",
    "book = str(text1)\n",
    "\n",
    "for text in [text1,text2,text3,text4,text5,text6,text7,text8,text9]:\n",
    "    count = text.count(word_to_count)\n",
    "    if count > maximum:\n",
    "        maximum = count\n",
    "        book = str(text)\n",
    "        \n",
    "print('The highest count for word ' + word_to_count+ ' is found in book ' + book +' and is equal to '+ str(maximum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I explain what happens in the code above by adding explanations using the # and adding print statements. Basically, we first assume the maximum value is zero and loop over the other texts. For each text, we check how often the word 'love' occurs. If our search term appears more often we replace the variable `maximum` with the value of the variable `count`. Then we go the next text. The variable `maximum` saves the highest values encountered so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_count = 'love' # define the word you want to\n",
    "\n",
    "maximum = 0 # assume maximum is zero\n",
    "book = '' # assume no book is found\n",
    "\n",
    "for text in [text1,text2,text3,text4,text5,text6,text7,text8,text9]: # loop over all texts\n",
    "    count = text.count(word_to_count) # count how often the word appears in this text\n",
    "    print('The word '+ word_to_count +' appears '+ str(count) + ' times in ' + str(text) )\n",
    "    print('The existing maximum value at this stage is ' + str(maximum) )\n",
    "    if count > maximum: # check if this count is higher as the current maximum value\n",
    "        print('\\n!!! Something important happening here. Replacing maximum value!')\n",
    "        print('Because ' + str(count) + ' > ' + str(maximum) + ' we replace the maximum value')\n",
    "        maximum = count # if so, replace the value for the variable maximum with the one that is stored in count\n",
    "        book = str(text) # this is just a nice addition, so we can later check which book\n",
    "        print('The maximum value is now equal to ' + str(maximum) + ' and is found in ' + str(text))\n",
    "        print('End of the if condition.\\n')\n",
    "    else:\n",
    "        print('Because ' + str(count) + ' < ' + str(maximum) + ' we do not replace the maximum value\\n')\n",
    "        \n",
    "print('The highest count for word ' + word_to_count+ ' is found in book ' + book +' and is equal to '+ str(maximum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the term to search for to 'war' and see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Which of books listed above has the highest lexical diversity? If we do not have a good guess for `maximum` we just take an arbitrary book and compare its lexical diversity to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_count = 'love' # define the word you want to\n",
    "\n",
    "maximum = lexical_diversity(text1) # assume maximum is equal to text1\n",
    "book = '' # assume no book is found\n",
    "\n",
    "for text in [text1,text2,text3,text4,text5,text6,text7,text8,text9]: # loop over all texts\n",
    "    div = lexical_diversity(text) \n",
    "    print('The existing maximum lexical diversity at this iteration is ' + str(maximum))\n",
    "    print(str(text) + ' has a lexical diversity of ',str(div)) \n",
    "    if div > maximum: # check if this count is higher as the current maximum value\n",
    "        print('\\n!!! Something important happening here. Replacing maximum value!')\n",
    "        print('Because ' + str(div) + ' > ' + str(maximum) + ' we replace the maximum value')\n",
    "        maximum = count # if so, replace the value for the variable maximum with the one that is stored in div\n",
    "        book = str(text) # this is just a nice addition, so we can later check which book\n",
    "        print('The maximum value is now equal to ' + str(maximum) + ' and is found in ' + str(text))\n",
    "        print('End of the if condition.\\n')\n",
    "    else:\n",
    "        print('Because ' + str(div) + ' < ' + str(maximum) + ' we do not replace the maximum value\\n')\n",
    "        \n",
    "print('The highest lexial diversity of ' + str(maximum) + ' is found in book ' + book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, you might wonder: what if I want to investigate *other* texts? Of course, this is possible but requires some *preprocessing* steps. We have to **tokenize** the document on your computer or on the Web (which is just a sequence of characters) to an NLTK `Text` object.\n",
    "\n",
    "Before we do this, let's inspects some of NLTK's preprocessing tools.\n",
    "\n",
    "In the previous lecture, we have already covered a few common preprocessing steps such a removing punctuation and lower casing. Here we will take a slightly different route because NLTK takes cares of many of issues that required these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is useful to process a text by sentence, if we want, for example, inquire the use of different words within its meaningful context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "book = 'This is a sentence. And this another one!'\n",
    "sent_tokenize(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: How many sentences does Tolstoy's 'War and Peace' contain (approximately)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "data = requests.get('http://www.gutenberg.org/files/2600/2600-0.txt').text\n",
    "sents = sent_tokenize(data)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alluded to earlier, 'tokens' are the minimal units for the machine to process. We often simply equated this with words, which, in turn, were defined as everything between to whitespaces--but the relationship is more complex. Luckily, NLTK comes with many ready-made tools for splitting strings into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"On the 12th of August, 18-- (just three days after my tenth birthday, when I had been given such wonderful presents), I was awakened at seveno’clock in the morning by Karl Ivanitch slapping the wall close to my head with a fly-flap made of sugar paper and a stick.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not one \"correct\" method for tokenizing texts. Therefore NLTK comes with many different tokenizers. What are their differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(regexp_tokenize(sentence, pattern='\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: How many tokens are there in 'War and Peace'? How many unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(data)\n",
    "print(len(tokens))\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: read in Hobbes' \"Leviathan\", lowercase the whole book, split it by sentence, and then tokenize each sentence into words.\n",
    "> To lowercase use the `lowercase()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/cache/epub/3207/pg3207.txt'\n",
    "data = requests.get(url).text\n",
    "sentences = sent_tokenize(data) # tokenize the text by sentence\n",
    "\n",
    "tokenized_sentences = [] # create an empty variable where we store the text tokenized first by sentence and then by word\n",
    "\n",
    "for sentence in sentences: # loop over all sentences\n",
    "    sentence_lower = sentence.lower() # lowercase the sentence\n",
    "    sentence_tokens = word_tokenize(sentence_lower) # tokenize the sentence\n",
    "    tokenized_sentences.append(sentence_tokens) # save the tokenized sentence to the list with the name tokenized_sentences\n",
    "    \n",
    "print(len(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few items in the tokenized_sentences list \n",
    "print(tokenized_sentences[100:150]) # each item is the list is again a list (see nested lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all sentences which discuss **man** and **God**.\n",
    "> Tip: to check for two conditions use the following logical expression:\n",
    "\n",
    ">l = ['a','b','c']\n",
    "\n",
    ">'b' in l and 'e' in l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "man_god = []\n",
    "for sentence in tokenized_sentences:\n",
    "    if ('man' in sentence) and ('god' in sentence):\n",
    "        man_god.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(man_god)) # how many sentences are there with man and god?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_god[50:55] # inspect these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in man_god: # make the results a bit more readable\n",
    "    print(' '.join(s))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all sentences which discuss **man**, or its plural **men**, and **God**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_god = []\n",
    "for sentence in tokenized_sentences:\n",
    "    if ('man' in sentence or 'men' in sentence) and ('god' in sentence):\n",
    "        men_god.append(sentence)\n",
    "print(len(men_god))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Texts: Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming, in its literal sense, amounts to cutting down the branches of a tree to its stem. But also tokens can be reduced to their stem. **Stemming is a crude, rule-based process by which we want to group together different variations of a token.** For example, the word 'eat' will have variations like 'eating', 'eaten', 'eats', and so on. In some applications, when does **not make sense to differentiate between 'eat' and 'eaten'**, we typically use stemming to reduce these grammatical variances to the root of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = PorterStemmer()\n",
    "print(pst.stem('loving'))\n",
    "print(pst.stem('loved'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above, we create a stemmer object and apply the `stem()` method to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize('love loved loving flower flowers dogs dog')\n",
    "stems = [pst.stem(w) for w in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Download the Bible, stem it, and count how often the stem 'love' appears (use the percentage function above). Compare this percentage with a non-stemmed version of the Bible.\n",
    "> Tip: use the count method\n",
    "\n",
    "> l = ['a','a','b','c']\n",
    "\n",
    "> l.count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_url = 'http://www.gutenberg.org/cache/epub/10/pg10.txt'\n",
    "\n",
    "text_lower = requests.get(bible_url).text.lower() # download and lowercase the Bible\n",
    "\n",
    "tokenized_bible = word_tokenize(text_lower) # tokenize the Bible by word\n",
    "\n",
    "stemmed_bible = [] # place to store the stemmed version of the bible\n",
    "\n",
    "for token in tokenized_bible: # iterate over each token in the tokenized Bible\n",
    "    \n",
    "    stem = pst.stem(token) # stem each token\n",
    "    stemmed_bible.append(stem) # append the stemmed token to the list with the name stemmed_bible\n",
    "    \n",
    "print(tokenized_bible.count('love'))\n",
    "print(stemmed_bible.count('love'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more **methodical** way of converting all the **grammatical/inflected** forms of the root of the word. Lemmatization uses context and **part of speech** (see below) to determine the inflected form of the word and applies **different normalization** rules for each part of speech to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wlem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wlem.lemmatize(\"flowers\",pos='n'))\n",
    "print(wlem.lemmatize(\"was\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wlem.lemmatize(\"run\",pos='v'))\n",
    "print(wlem.lemmatize(\"ran\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pst.stem('run'))\n",
    "print(pst.stem('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(wlem.lemmatize(\"mouse\",pos='n'))\n",
    "print(wlem.lemmatize(\"mice\",pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pst.stem('mouse'))\n",
    "print(pst.stem('mice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### From File to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interrogate text outside of the NLTK corpus. You only need to tokenize the text first. Let's download Tolstoy's \"Childhood\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "url = 'http://www.gutenberg.org/files/2142/2142-0.txt'\n",
    "text = requests.get(url).text\n",
    "tokens = word_tokenize(text)\n",
    "nltk_text = nltk.text.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can apply all the NLTK methods to this book! Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: apply the `concordance`, `similar`, `collocation` and `dispersion_plot` to this book (or another book of your choice, preferably one you are familiar with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('childhood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words made easy (look at 4.2 Dictionaries for more explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = 'We see, therefore, how the modern bourgeoisie is itself the product of a long course of development, of a series of revolutions in the modes of production and of exchange.'\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "fd = FreqDist(tokens)\n",
    "print(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd['production']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a part of speech tag to each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "tokens = word_tokenize(\"And now for something completely different\")\n",
    "tagged = pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to iterate over a list. The output of `pos_tag` returns a list of **tuples** which we can **unpack** using the following notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_element = tagged[0]\n",
    "print('Tuple = ',first_element)\n",
    "word,tag = first_element\n",
    "print('Word = ',word)\n",
    "print('Tag = ',tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word,tag in tagged:\n",
    "    print('This is a word',word)\n",
    "    print('This is a tag',tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar but slightly more elegant than:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for element in tagged:\n",
    "    print('This is a word',element[0])\n",
    "    print('This is a tag',element[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you are free to choose the variable names, but it should match the number of items in each tuple. For example, this will raise a `ValueError`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,tag,something_else in tagged:\n",
    "    print('This is a word',word)\n",
    "    print('This is a tag',tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g. `nltk.help.upenn_tagset('RB')`. An overview of all the Part-of-Speech tags you'll find [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what does 'RB' mean\n",
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Collect all the nouns in \"The Communist Manifesto\" of Marx and Engels.\n",
    "> Tip use the `startswith()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/cache/epub/61/pg61.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = requests.get(url).text # load the text\n",
    "\n",
    "tokens = word_tokenize(text) # tokenize the text\n",
    "\n",
    "pos_tagged = pos_tag(tokens) # part of speech tag the tokenized text, this can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_tagged[100:120]) # inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for word,tag in pos_tagged:\n",
    "    if tag.startswith('NN'):\n",
    "        nouns.append(word)\n",
    "    \n",
    "print(len(nouns))\n",
    "print(nouns[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difficult Exercise**: Collect all [bigrams](https://en.wikipedia.org/wiki/Bigram) (sequence of two words) that start with an adjective and end with a noun.\n",
    "> Tip: Use index notation as shown below (below I apply it to string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'A spectre is haunting Europe.'\n",
    "print(len(sentence))\n",
    "print(range(len(sentence)))\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentence)-1):\n",
    "    print(sentence[i],sentence[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Additional advice: use the following code in combination with the logical `and` operator:\n",
    "\n",
    "> example = ('common', 'JJ')\n",
    "\n",
    ">'JJ' in example[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "for i in range(len(pos_tagged)-1):\n",
    "    if pos_tagged[i][1].startswith('JJ') and pos_tagged[i+1][1].startswith('NN'):\n",
    "        phrases.append(pos_tagged[i:i+2])\n",
    "    \n",
    "print(len(phrases))\n",
    "print(phrases[90:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also has functions for more refined syntactic analysis and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Mark and John are working at Google.\"\n",
    "ne= ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "ne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
