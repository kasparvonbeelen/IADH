{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors and Matrices: Basic Text Mining Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: A (Very) Advanced Example: Topic Modeling\n",
    "\n",
    "This example is a sneak peak into the more advanced features of data mining. We will try find topics in the King James Bible. Many of the features shown below will be covered in more detail later in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "#text = requests.get('http://www.gutenberg.org/files/1081/1081-0.txt').text.lower()\n",
    "text = requests.get('http://www.gutenberg.org/cache/epub/1041/pg1041.txt').text.lower()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We study the book by approximately paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we split by two hard returns\n",
    "poems = text.split('\\r\\n\\r\\n')\n",
    "print(len(poems))\n",
    "print(poems[301])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sonnets start here\n",
    "print(poems[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sonnets end here\n",
    "print(poems[-55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then we properly tokenize the each sonnet, i.e. separate words from punctuation (for more details, see next lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence = 'This example, will be: properly tokenized!'\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we do many things:\n",
    "    - Tokenize each sonnet\n",
    "    - Select longer sonnets, more than 5 tokens and store them in a variable long_poems\n",
    "    - Retain only alphabetical items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code is rather hacky, but hey it works!\n",
    "long_poems = []\n",
    "\n",
    "for poem in poems[17:-55]: # ignore the gutenberg appendices\n",
    "    \n",
    "    tokens = word_tokenize(poem) # seperate words form interpunction\n",
    "\n",
    "    alpha_tokens = [] # here we will store all alphabetic items\n",
    "        \n",
    "    for token in tokens: # iterate over the tokenized version of the sonnet\n",
    "        if token.isalpha(): # if the token is alphabetic\n",
    "            alpha_tokens.append(token) # append it to alpha_tokens\n",
    "        \n",
    "    if len(alpha_tokens) >= 5: # if we have more than 5 alphabetic tokens\n",
    "        long_poems.append(alpha_tokens) # add the sonnet to long poems\n",
    "            \n",
    "print('We selected ',len(long_poems),' books of the total of ', len(poems)) # print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demonstrate what happens in the loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed our data to Topic Modeling algorithm, we have to make a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix), for this we need to have a list with all types (distinct words) often called a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize the whole text at one\n",
    "all_tokens = set(word_tokenize(text))\n",
    "len(all_tokens)\n",
    "len(set(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set()` transforms a list to unordered set and thereby removes all duplicates as in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [1,1,1,2,3,4,4]\n",
    "print(l)\n",
    "s = set(l)\n",
    "print(s)\n",
    "l = list(s)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as\n",
    "list(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the hundred first items of list(set())\n",
    "print(list(all_tokens)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function words are often discarded. This can be easily done using a membership condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopw = stopwords.words('english') # get a list of stop words from an external library\n",
    "print(stopw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for w in all_tokens: # iterate over all the tokens in all_tokens\n",
    "    if w.isalpha() and w not in stopw: # if an items is alphanumeric \n",
    "        vocab.append(w)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vocab` now contains all alphabetic words that are not stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will transform all the titles to a **binary vector**: a list where each value indicates if a word appears (1) or not (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's take a random example\n",
    "example = long_poems[30]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = []\n",
    "for w in vocab:\n",
    "    if w in example:\n",
    "        vector.append(1)\n",
    "    else:\n",
    "        vector.append(0)\n",
    "\n",
    "len(vector) == len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vector[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the document-term matrix is a vector representing one sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform our corpus to a document term matrix: A matrix where the rows represent songs, and columns the presence of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for tokens in long_poems:\n",
    "    vector = []\n",
    "    set_tokens = set(tokens)\n",
    "    for w in vocab:\n",
    "        if w in set_tokens:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    vectors.append(vector)\n",
    "            \n",
    "print(len(vectors) == len(long_poems))\n",
    "print(len(vectors[0]) == len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a topic model on the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10, max_iter=10, #maybe n_components\n",
    "                                learning_method='online',\n",
    "                                random_state=0,\n",
    "                                verbose=1,\n",
    "                                n_jobs=1)\n",
    "lda.fit(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the different topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example takes from: http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(lda, list(vocab), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises DIY: Loops, Conditions and Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a list, say for example this one:\n",
    "\n",
    "  `a = [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]`\n",
    "\n",
    "and write a program that prints out all the elements of the list that are less than 5.\n",
    "\n",
    "Extras:\n",
    "\n",
    "- Instead of printing the elements one by one, make a new list that has all the elements less than 5 from this list in it and print out this new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ex. 6: Write code that multiplies all items in the list \n",
    "    \n",
    "`a = [1,2,3,4,5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1*2*3*4*5==result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ex. 7: Write a Python program to get the smallest number from a list of integers. \n",
    "    \n",
    "`a = [6,9,4,2,7,8,9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ex. 8: Write a Python program to get the highest number from a list of integers. \n",
    "    \n",
    "`a = [6,9,4,2,7,8,9]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ex. 9: Write a Python program that prints the longest word in the following sentence:\n",
    "\n",
    "`sentence = \"Zunächst ein etwas abgenutzter Koffer aus weißem Leder, dem man es ansah, daß er nicht zum erstenmal eine Reise machte.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
