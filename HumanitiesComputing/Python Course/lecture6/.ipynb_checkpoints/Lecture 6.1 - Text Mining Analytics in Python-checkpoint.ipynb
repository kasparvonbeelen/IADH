{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors and Matrices: Text Mining in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook serves as an introduction to the more advanced features of text mining. We will apply topic modelling, clustering and supervised classification to speeches taken from the records of the British House of Commons. \n",
    "\n",
    "The dataset below contains a CSV file in which each line records a mention of immigration, with associated metadata (party, date etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data with Pandas from the disk into our Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the pandas library`\n",
    "import pandas as pd\n",
    "\n",
    "dateparse = lambda x: pd.datetime.strptime(x,'%Y-%m-%d')\n",
    "\n",
    "# read the CSV file as assign \n",
    "df = pd.read_csv('data/immigration_uk.csv',\n",
    "                       header=0, # specify where the header is located\n",
    "                       sep=\",\", # specify the delimiter\n",
    "                       # additional \n",
    "                       escapechar=u'\\\\', # quotes inside the text are escaped\n",
    "                       parse_dates=['date'], # which column contains dates\n",
    "                       date_parser = dateparse # how to read dates\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.head()` method prints the first lines of table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the Pandas syntax to select specific rows:\n",
    "    - speeches from Labour and Conservative speakers\n",
    "    - speches after 2005-05-04\n",
    "    \n",
    "`.shape` returns the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = df[df.party.isin(['Labour','Conservative']) & (df.date >= '2005-05-04')]\n",
    "print(df.shape)\n",
    "print(df_red.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert two columns `text` and `party` to a Python list using the `list()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = list(df_red.text)\n",
    "parties = list(df_red.party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then we properly tokenize the each speech, i.e. separate words and delete punctuation as in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "sentence = 'This example, will be: properly tokenized!'\n",
    "tokens = regexp_tokenize(sentence,pattern='\\w+')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply tokenization to each speech and save it in the list `tokenized_speeches`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is rather hacky, but hey it works!\n",
    "tokenized_speeches = []\n",
    "\n",
    "\n",
    "for speech in texts: \n",
    "    speech_lower = speech.lower() # lowercase each book\n",
    "    tokens = regexp_tokenize(speech_lower,pattern='\\w+') # seperate words \n",
    "    tokenized_speeches.append(tokens) # add the sonnet to long poems\n",
    "    \n",
    "            \n",
    "print(len(tokenized_speeches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic modelling, and other algorithms, require a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) as input.\n",
    "This matrix is a table where each row represents a document, and a each column represents the frequency of one specific work.\n",
    "\n",
    "To create such a matrix we have to list all types (distinct words), which is often called a **vocabulary**. The vocabulary will define the column names (e.g. each column keeps track of how often one word appears accross *all* documents.\n",
    "\n",
    "To create the vocabulary we first join all the speeches, tokenize it, and use the Python `set()` function to remove all duplicates. This returns a set with all the types in the corpus.\n",
    "\n",
    "As you can see the number of tokens is way smaller than the number of types (here defined by the variable `vocab`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the whole text at one\n",
    "all_tokens = regexp_tokenize(' '.join(texts).lower(),pattern='\\w+')\n",
    "print(len(all_tokens))\n",
    "vocab = set(all_tokens)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said earlier `set()` transforms a list to unordered set and thereby removes all duplicates as in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the hundred first items of list(set())\n",
    "print(list(all_tokens)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text mining text stop words are often discarded as they are not considered informative for the specific task. Removing stop words can be easily done using a membership condition in combination with the NLTK `stopwords` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopw = stopwords.words('english') # get a list of stop words from an external library\n",
    "print(stopw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_f = []\n",
    "print(len(vocab))\n",
    "for w in vocab: # iterate over all the tokens in all_tokens\n",
    "    if w not in stopw: # if an items is alphanumeric \n",
    "        vocab_f.append(w)\n",
    "print(len(vocab_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach would be to remove word that appear in more or less than a number of documents. To do this, we first map each word to its document frequency. The threshold we set for removing words are arbitrary, but motivated by the intuition that we want to ignore words that are too frequent or too rare, as these will mess up our models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unique_tokens_all = [] # we we store the types of each document\n",
    "\n",
    "for speech in tokenized_speeches: # go over all the speeches\n",
    "    unique_tokens_speech = list(set(speech)) # make list from the set of unique words in a book\n",
    "    unique_tokens_all.extend(unique_tokens_speech) # add the words to the list unique_words_all, not that we extend() and not append()\n",
    "\n",
    "doc_freq = Counter(unique_tokens_all)  # use Counter to convert list to a document frequency\n",
    "doc_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the vocab filtering. You'll notice that this time the vocabulary is heavily reduced. \n",
    "To set a value for the maximum document frequency, we multiplicate the number of documents by 0.75 (i.e. divide by 100 and multiply by 75 so we keep words that appear in less than 75% of all the speeches) and then cast this float as an integer using `int()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_f = []\n",
    "upper = int(len(texts)*0.75)\n",
    "print('Maximum document frequency = ' + str(upper))\n",
    "\n",
    "for w in vocab: # iterate over all the tokens in all_tokens\n",
    "    if doc_freq.get(w,0) > 5 and doc_freq.get(w,0) < upper and w.isalpha(): # if an items is alphanumeric \n",
    "        vocab_f.append(w)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of vocabulary before filtering = '+ str(len(vocab)))\n",
    "print('Length of vocabulary before filtering = '+ str(len(vocab_f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example of the words in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_f[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will transform all the books to a **count vector**: a list where each value indicates how often a word appears (int) or not (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a random example\n",
    "example = tokenized_speeches[30]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the word counts of the this segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_s = Counter(example)\n",
    "print(fd_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over our filtered vocabulary, for each word in the vocabulary, we get the frequency of that word in our example text. The size of the vector is naturally equal to the number of words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = []\n",
    "for w in vocab_f:\n",
    "    count = fd_s.get(w,0)\n",
    "    vector.append(count)\n",
    "print(len(vector))\n",
    "print(len(vector) == len(vocab_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector is actually nothing more than a list with a lot of zeros and a few integers that tell us how often a word at position x in our vocab appears. A vector simply represents the word counts of a document with respect to a certain vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector) # prints the total vector for all the sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_f[40]) # the word \"part\" at position 40\n",
    "print(vector[40]) # does not appeat in the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform our corpus to a **document-term-matrix**: A matrix where the rows represent songs, and columns the presence of a word. The matrix is nothing more than a nested list: a list to which we append all the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [] # create an empty list that will serve as a matrix. here we store our vectors\n",
    "for tokens in tokenized_speeches:\n",
    "    vector = [] # create an empty vector\n",
    "    freqdist = Counter(tokens) # compute word frequencies\n",
    "    \n",
    "    for w in vocab_f: # iterate over the vocabulary    \n",
    "        vector.append(freqdist.get(w,0)) # append the word frequency for word w otherwise append zero\n",
    "    matrix.append(vector) # append vector to the matrix\n",
    "            \n",
    "print(len(matrix) == len(tokenized_speeches)) # number of rows in the matrix is equal to the number of books\n",
    "print(len(matrix[0]) == len(vocab_f)) # dimensions of the vector is equal to the total vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Once we have this document term matrix, we can feed our data to other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "\n",
    "We can discover thematical clusters using Hierarchical Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "dm = pdist(matrix[:1000]) # we reduce our data, otherwise it does not fit on the plot\n",
    "Z = hierarchy.ward(dm) # convert document term matrix to a distance matrix which how close each vector pair is\n",
    "plt.figure(figsize=(20,8)) # we make the figure a bit bigger\n",
    "dn = hierarchy.dendrogram(Z) # we plot the dendogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (Topic Modelling)\n",
    "<img src=\"http://wiki.ubc.ca/images/thumb/1/15/Illustrating_LDA.jpg/700px-Illustrating_LDA.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a topic model on the document-term matrix, which aims to recover the latent topics in a corpus. The algorithm is sensitive to document frequency threshold and the number of topics you select. You can play a bit with these parameters and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=20, #maybe n_components, this defines the number of topics we want to find\n",
    "                                max_iter=10, # how often should the algorithm iterate over the corpus\n",
    "                                learning_method='online',\n",
    "                                random_state=0, # in order to be able to reproduce the results\n",
    "                                verbose=5, # how many prints during training?\n",
    "                                n_jobs=5) # set the number of cores Python can use, makes the training faster\n",
    "lda.fit(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below helps you to print the different topics, which are a list of words on each line, words that tend to co-occur in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example takes from: http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print_top_words(lda, vocab_f, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Supervised Classification](https://en.wikipedia.org/wiki/Supervised_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build a classifier that can detect whether a speech is Left of Right. For this we use supervised classification.\n",
    "\n",
    "For supervised classification we need labeled data, in this case each speech has a party associated with the text.\n",
    "\n",
    "Building a classifier model, broadly consists of two steps:\n",
    "**Training**: we give the computer some examples of texts with correct labels. From these examples it can learn a relation between certain words and the label, such \"illegal\" appears more in Conservative speeches.\n",
    "**Testing**: we test the model on new examples, and assess how well the computer can guess or predict the labels of the texts we asked it to classify.\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score,accuracy_score,classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "y = np.array([1 if p=='Labour' else 0 for p in parties]) # y is the vector of labels, 0 is Conservative, 1 is Labour\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    matrix, y, test_size=0.33, random_state=42) # we divide the data into a training (66%) and test set (33%)\n",
    "\n",
    "classifier = LinearSVC(class_weight='balanced',C=1)  # we create a classifier\n",
    "classifier.fit(X_train,y_train) # we fit the model or train the classifier\n",
    "predictions = classifier.predict(X_test) # we let it predict the labels of new examples\n",
    "print(f1_score(y_test,predictions),accuracy_score(y_test,predictions)) # we compute how well it could predict the labels of new instances.\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(feature_names, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    top10 = np.argsort(clf.coef_[0])[-10:]\n",
    "    print(\"%s: %s\" % ('Labour',\n",
    "              \" \".join(feature_names[j] for j in top10)))\n",
    "    top10 = np.argsort(clf.coef_[0])[:10]\n",
    "    print(\"%s: %s\" % ('Conservative',\n",
    "              \" \".join(feature_names[j] for j in top10)))\n",
    "        \n",
    "print_top10(vocab_f, classifier, [0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
