{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize, regexp_tokenize\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.sentiment import vader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_alice = \"\"\"ALICE was beginning to get very tired of sitting by her\n",
    "sister on the bank, and of having nothing to do: once or twice she had\n",
    "peeped into the book her sister was reading, but it had no pictures or\n",
    "conversations in it, \"and what is the use of a book,\" thought Alice,\n",
    "\"without pictures or conversations?\"\n",
    "\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "hot day made her feel very sleepy and stupid) whether the pleasure of\n",
    "making a daisy-chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice was beginning to get very tired of sitting by her\n",
      "sister on the bank, and of having nothing to do: once or twice she had\n",
      "peeped into the book her sister was reading, but it had no pictures or\n",
      "conversations in it, \"and what is the use of a book,\" thought alice,\n",
      "\"without pictures or conversations?\"\n",
      "\n",
      "so she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid) whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a white rabbit with pink eyes ran\n",
      "close by her.\n"
     ]
    }
   ],
   "source": [
    "text = text_alice.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Units of Analysis\n",
    "\n",
    "#### Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'l', 'i', 'c', 'e', 'w', 'a', 's', 'b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', 't', 'o', 'g', 'e', 't', 'v', 'e', 'r', 'y', 't', 'i', 'r', 'e', 'd', 'o', 'f', 's', 'i', 't', 't', 'i', 'n', 'g', 'b', 'y', 'h', 'e', 'r', 's', 'i', 's', 't', 'e', 'r', 'o', 'n', 't', 'h', 'e', 'b', 'a', 'n', 'k', 'a', 'n', 'd', 'o', 'f', 'h', 'a', 'v', 'i', 'n', 'g', 'n', 'o', 't', 'h', 'i', 'n', 'g', 't', 'o', 'd', 'o', 'o', 'n', 'c', 'e', 'o', 'r', 't', 'w', 'i', 'c', 'e', 's', 'h', 'e', 'h', 'a', 'd', 'p', 'e', 'e', 'p', 'e', 'd', 'i', 'n', 't', 'o', 't', 'h', 'e', 'b', 'o', 'o', 'k', 'h', 'e', 'r', 's', 'i', 's', 't', 'e', 'r', 'w', 'a', 's', 'r', 'e', 'a', 'd', 'i', 'n', 'g', 'b', 'u', 't', 'i', 't', 'h', 'a', 'd', 'n', 'o', 'p', 'i', 'c', 't', 'u', 'r', 'e', 's', 'o', 'r', 'c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', 's', 'i', 'n', 'i', 't', 'a', 'n', 'd', 'w', 'h', 'a', 't', 'i', 's', 't', 'h', 'e', 'u', 's', 'e', 'o', 'f', 'a', 'b', 'o', 'o', 'k', 't', 'h', 'o', 'u', 'g', 'h', 't', 'a', 'l', 'i', 'c', 'e', 'w', 'i', 't', 'h', 'o', 'u', 't', 'p', 'i', 'c', 't', 'u', 'r', 'e', 's', 'o', 'r', 'c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', 's', 's', 'o', 's', 'h', 'e', 'w', 'a', 's', 'c', 'o', 'n', 's', 'i', 'd', 'e', 'r', 'i', 'n', 'g', 'i', 'n', 'h', 'e', 'r', 'o', 'w', 'n', 'm', 'i', 'n', 'd', 'a', 's', 'w', 'e', 'l', 'l', 'a', 's', 's', 'h', 'e', 'c', 'o', 'u', 'l', 'd', 'f', 'o', 'r', 't', 'h', 'e', 'h', 'o', 't', 'd', 'a', 'y', 'm', 'a', 'd', 'e', 'h', 'e', 'r', 'f', 'e', 'e', 'l', 'v', 'e', 'r', 'y', 's', 'l', 'e', 'e', 'p', 'y', 'a', 'n', 'd', 's', 't', 'u', 'p', 'i', 'd', 'w', 'h', 'e', 't', 'h', 'e', 'r', 't', 'h', 'e', 'p', 'l', 'e', 'a', 's', 'u', 'r', 'e', 'o', 'f', 'm', 'a', 'k', 'i', 'n', 'g', 'a', 'd', 'a', 'i', 's', 'y', 'c', 'h', 'a', 'i', 'n', 'w', 'o', 'u', 'l', 'd', 'b', 'e', 'w', 'o', 'r', 't', 'h', 't', 'h', 'e', 't', 'r', 'o', 'u', 'b', 'l', 'e', 'o', 'f', 'g', 'e', 't', 't', 'i', 'n', 'g', 'u', 'p', 'a', 'n', 'd', 'p', 'i', 'c', 'k', 'i', 'n', 'g', 't', 'h', 'e', 'd', 'a', 'i', 's', 'i', 'e', 's', 'w', 'h', 'e', 'n', 's', 'u', 'd', 'd', 'e', 'n', 'l', 'y', 'a', 'w', 'h', 'i', 't', 'e', 'r', 'a', 'b', 'b', 'i', 't', 'w', 'i', 't', 'h', 'p', 'i', 'n', 'k', 'e', 'y', 'e', 's', 'r', 'a', 'n', 'c', 'l', 'o', 's', 'e', 'b', 'y', 'h', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "characters = [w for w in text if w.isalpha()]\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', 56), ('i', 40), ('t', 40), ('o', 36), ('n', 35), ('s', 32), ('a', 31), ('h', 31), ('r', 26), ('d', 21), ('w', 14), ('c', 13), ('g', 13), ('u', 13), ('l', 12), ('b', 11), ('p', 10), ('y', 9), ('f', 7), ('k', 6), ('v', 5), ('m', 3)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(characters).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "tokens = regexp_tokenize(text,pattern='\\w+')\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'alice', 'without', 'pictures', 'or', 'conversations', 'so', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', 'as', 'well', 'as', 'she', 'could', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', 'when', 'suddenly', 'a', 'white', 'rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 3,\n",
       "         'alice': 2,\n",
       "         'and': 4,\n",
       "         'as': 2,\n",
       "         'bank': 1,\n",
       "         'be': 1,\n",
       "         'beginning': 1,\n",
       "         'book': 2,\n",
       "         'but': 1,\n",
       "         'by': 2,\n",
       "         'chain': 1,\n",
       "         'close': 1,\n",
       "         'considering': 1,\n",
       "         'conversations': 2,\n",
       "         'could': 1,\n",
       "         'daisies': 1,\n",
       "         'daisy': 1,\n",
       "         'day': 1,\n",
       "         'do': 1,\n",
       "         'eyes': 1,\n",
       "         'feel': 1,\n",
       "         'for': 1,\n",
       "         'get': 1,\n",
       "         'getting': 1,\n",
       "         'had': 2,\n",
       "         'having': 1,\n",
       "         'her': 5,\n",
       "         'hot': 1,\n",
       "         'in': 2,\n",
       "         'into': 1,\n",
       "         'is': 1,\n",
       "         'it': 2,\n",
       "         'made': 1,\n",
       "         'making': 1,\n",
       "         'mind': 1,\n",
       "         'no': 1,\n",
       "         'nothing': 1,\n",
       "         'of': 5,\n",
       "         'on': 1,\n",
       "         'once': 1,\n",
       "         'or': 3,\n",
       "         'own': 1,\n",
       "         'peeped': 1,\n",
       "         'picking': 1,\n",
       "         'pictures': 2,\n",
       "         'pink': 1,\n",
       "         'pleasure': 1,\n",
       "         'rabbit': 1,\n",
       "         'ran': 1,\n",
       "         'reading': 1,\n",
       "         'she': 3,\n",
       "         'sister': 2,\n",
       "         'sitting': 1,\n",
       "         'sleepy': 1,\n",
       "         'so': 1,\n",
       "         'stupid': 1,\n",
       "         'suddenly': 1,\n",
       "         'the': 7,\n",
       "         'thought': 1,\n",
       "         'tired': 1,\n",
       "         'to': 2,\n",
       "         'trouble': 1,\n",
       "         'twice': 1,\n",
       "         'up': 1,\n",
       "         'use': 1,\n",
       "         'very': 2,\n",
       "         'was': 3,\n",
       "         'well': 1,\n",
       "         'what': 1,\n",
       "         'when': 1,\n",
       "         'whether': 1,\n",
       "         'white': 1,\n",
       "         'with': 1,\n",
       "         'without': 1,\n",
       "         'worth': 1,\n",
       "         'would': 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams\n",
    "\n",
    "Example of [Google Ngram Viewer](https://books.google.com/ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a_book_thought': 1,\n",
       "         'a_daisy_chain': 1,\n",
       "         'a_white_rabbit': 1,\n",
       "         'alice_was_beginning': 1,\n",
       "         'alice_without_pictures': 1,\n",
       "         'and_of_having': 1,\n",
       "         'and_picking_the': 1,\n",
       "         'and_stupid_whether': 1,\n",
       "         'and_what_is': 1,\n",
       "         'as_she_could': 1,\n",
       "         'as_well_as': 1,\n",
       "         'bank_and_of': 1,\n",
       "         'be_worth_the': 1,\n",
       "         'beginning_to_get': 1,\n",
       "         'book_her_sister': 1,\n",
       "         'book_thought_alice': 1,\n",
       "         'but_it_had': 1,\n",
       "         'by_her_sister': 1,\n",
       "         'chain_would_be': 1,\n",
       "         'close_by_her': 1,\n",
       "         'considering_in_her': 1,\n",
       "         'conversations_in_it': 1,\n",
       "         'conversations_so_she': 1,\n",
       "         'could_for_the': 1,\n",
       "         'daisies_when_suddenly': 1,\n",
       "         'daisy_chain_would': 1,\n",
       "         'day_made_her': 1,\n",
       "         'do_once_or': 1,\n",
       "         'eyes_ran_close': 1,\n",
       "         'feel_very_sleepy': 1,\n",
       "         'for_the_hot': 1,\n",
       "         'get_very_tired': 1,\n",
       "         'getting_up_and': 1,\n",
       "         'had_no_pictures': 1,\n",
       "         'had_peeped_into': 1,\n",
       "         'having_nothing_to': 1,\n",
       "         'her_feel_very': 1,\n",
       "         'her_own_mind': 1,\n",
       "         'her_sister_on': 1,\n",
       "         'her_sister_was': 1,\n",
       "         'hot_day_made': 1,\n",
       "         'in_her_own': 1,\n",
       "         'in_it_and': 1,\n",
       "         'into_the_book': 1,\n",
       "         'is_the_use': 1,\n",
       "         'it_and_what': 1,\n",
       "         'it_had_no': 1,\n",
       "         'made_her_feel': 1,\n",
       "         'making_a_daisy': 1,\n",
       "         'mind_as_well': 1,\n",
       "         'no_pictures_or': 1,\n",
       "         'nothing_to_do': 1,\n",
       "         'of_a_book': 1,\n",
       "         'of_getting_up': 1,\n",
       "         'of_having_nothing': 1,\n",
       "         'of_making_a': 1,\n",
       "         'of_sitting_by': 1,\n",
       "         'on_the_bank': 1,\n",
       "         'once_or_twice': 1,\n",
       "         'or_conversations_in': 1,\n",
       "         'or_conversations_so': 1,\n",
       "         'or_twice_she': 1,\n",
       "         'own_mind_as': 1,\n",
       "         'peeped_into_the': 1,\n",
       "         'picking_the_daisies': 1,\n",
       "         'pictures_or_conversations': 2,\n",
       "         'pink_eyes_ran': 1,\n",
       "         'pleasure_of_making': 1,\n",
       "         'rabbit_with_pink': 1,\n",
       "         'ran_close_by': 1,\n",
       "         'reading_but_it': 1,\n",
       "         'she_could_for': 1,\n",
       "         'she_had_peeped': 1,\n",
       "         'she_was_considering': 1,\n",
       "         'sister_on_the': 1,\n",
       "         'sister_was_reading': 1,\n",
       "         'sitting_by_her': 1,\n",
       "         'sleepy_and_stupid': 1,\n",
       "         'so_she_was': 1,\n",
       "         'stupid_whether_the': 1,\n",
       "         'suddenly_a_white': 1,\n",
       "         'the_bank_and': 1,\n",
       "         'the_book_her': 1,\n",
       "         'the_daisies_when': 1,\n",
       "         'the_hot_day': 1,\n",
       "         'the_pleasure_of': 1,\n",
       "         'the_trouble_of': 1,\n",
       "         'the_use_of': 1,\n",
       "         'thought_alice_without': 1,\n",
       "         'tired_of_sitting': 1,\n",
       "         'to_do_once': 1,\n",
       "         'to_get_very': 1,\n",
       "         'trouble_of_getting': 1,\n",
       "         'twice_she_had': 1,\n",
       "         'up_and_picking': 1,\n",
       "         'use_of_a': 1,\n",
       "         'very_sleepy_and': 1,\n",
       "         'very_tired_of': 1,\n",
       "         'was_beginning_to': 1,\n",
       "         'was_considering_in': 1,\n",
       "         'was_reading_but': 1,\n",
       "         'well_as_she': 1,\n",
       "         'what_is_the': 1,\n",
       "         'when_suddenly_a': 1,\n",
       "         'whether_the_pleasure': 1,\n",
       "         'white_rabbit_with': 1,\n",
       "         'with_pink_eyes': 1,\n",
       "         'without_pictures_or': 1,\n",
       "         'worth_the_trouble': 1,\n",
       "         'would_be_worth': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "text_ngrams = ['_'.join(ngram) for ngram in ngrams(tokens, n)]\n",
    "Counter(text_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Complexity\n",
    "\n",
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = 'The Hamburgers prefer to eat their hamburgers without Ketchup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Hamburgers': 1,\n",
       "         'Ketchup': 1,\n",
       "         'The': 1,\n",
       "         'eat': 1,\n",
       "         'hamburgers': 1,\n",
       "         'prefer': 1,\n",
       "         'their': 1,\n",
       "         'to': 1,\n",
       "         'without': 1})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(example.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'eat': 1,\n",
       "         'hamburgers': 2,\n",
       "         'ketchup': 1,\n",
       "         'prefer': 1,\n",
       "         'the': 1,\n",
       "         'their': 1,\n",
       "         'to': 1,\n",
       "         'without': 1})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(example.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "print(pst.stem('loving'))\n",
    "print(pst.stem('loved'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'love', 'love', 'flower', 'flower', 'dog', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print([pst.stem(token) for token in word_tokenize('love loved loving flower flowers dogs dog')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ran', 'run', 'run']\n"
     ]
    }
   ],
   "source": [
    "print([pst.stem(token) for token in word_tokenize('ran run running')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pst.stem('mouse'))\n",
    "print(pst.stem('mice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alic',\n",
       " 'wa',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'get',\n",
       " 'veri',\n",
       " 'tire',\n",
       " 'of',\n",
       " 'sit',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'and',\n",
       " 'of',\n",
       " 'have',\n",
       " 'noth',\n",
       " 'to',\n",
       " 'do',\n",
       " 'onc',\n",
       " 'or',\n",
       " 'twice',\n",
       " 'she',\n",
       " 'had',\n",
       " 'peep',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'wa',\n",
       " 'read',\n",
       " 'but',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictur',\n",
       " 'or',\n",
       " 'convers',\n",
       " 'in',\n",
       " 'it',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'a',\n",
       " 'book',\n",
       " 'thought',\n",
       " 'alic',\n",
       " 'without',\n",
       " 'pictur',\n",
       " 'or',\n",
       " 'convers',\n",
       " 'so',\n",
       " 'she',\n",
       " 'wa',\n",
       " 'consid',\n",
       " 'in',\n",
       " 'her',\n",
       " 'own',\n",
       " 'mind',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she',\n",
       " 'could',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hot',\n",
       " 'day',\n",
       " 'made',\n",
       " 'her',\n",
       " 'feel',\n",
       " 'veri',\n",
       " 'sleepi',\n",
       " 'and',\n",
       " 'stupid',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'pleasur',\n",
       " 'of',\n",
       " 'make',\n",
       " 'a',\n",
       " 'daisi',\n",
       " 'chain',\n",
       " 'would',\n",
       " 'be',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'troubl',\n",
       " 'of',\n",
       " 'get',\n",
       " 'up',\n",
       " 'and',\n",
       " 'pick',\n",
       " 'the',\n",
       " 'daisi',\n",
       " 'when',\n",
       " 'suddenli',\n",
       " 'a',\n",
       " 'white',\n",
       " 'rabbit',\n",
       " 'with',\n",
       " 'pink',\n",
       " 'eye',\n",
       " 'ran',\n",
       " 'close',\n",
       " 'by',\n",
       " 'her']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pst.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wlem = WordNetLemmatizer()\n",
    "print(wlem.lemmatize(\"flowers\",pos='n'))\n",
    "print(wlem.lemmatize(\"was\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(wlem.lemmatize(\"run\",pos='v'))\n",
    "print(wlem.lemmatize(\"ran\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "print(wlem.lemmatize(\"mouse\",pos='n'))\n",
    "print(wlem.lemmatize(\"mice\",pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mous\n",
      "mice\n"
     ]
    }
   ],
   "source": [
    "print(pst.stem('mouse'))\n",
    "print(pst.stem('mice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ALICE', 'NNP'), ('was', 'VBD'), ('beginning', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('very', 'RB'), ('tired', 'JJ'), ('of', 'IN'), ('sitting', 'VBG'), ('by', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), ('on', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('of', 'IN'), ('having', 'VBG'), ('nothing', 'NN'), ('to', 'TO'), ('do', 'VB'), (':', ':'), ('once', 'RB'), ('or', 'CC'), ('twice', 'VB'), ('she', 'PRP'), ('had', 'VBD'), ('peeped', 'VBN'), ('into', 'IN'), ('the', 'DT'), ('book', 'NN'), ('her', 'PRP$'), ('sister', 'NN'), ('was', 'VBD'), ('reading', 'VBG'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('had', 'VBD'), ('no', 'DT'), ('pictures', 'NNS'), ('or', 'CC'), ('conversations', 'NNS'), ('in', 'IN'), ('it', 'PRP'), (',', ','), ('``', '``'), ('and', 'CC'), ('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('a', 'DT'), ('book', 'NN'), (',', ','), (\"''\", \"''\"), ('thought', 'VBD'), ('Alice', 'NNP'), (',', ','), (\"''\", \"''\"), ('without', 'IN'), ('pictures', 'NNS'), ('or', 'CC'), ('conversations', 'NNS'), ('?', '.'), (\"''\", \"''\"), ('So', 'IN'), ('she', 'PRP'), ('was', 'VBD'), ('considering', 'VBG'), ('in', 'IN'), ('her', 'PRP$'), ('own', 'JJ'), ('mind', 'NN'), ('(', '('), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('she', 'PRP'), ('could', 'MD'), (',', ','), ('for', 'IN'), ('the', 'DT'), ('hot', 'JJ'), ('day', 'NN'), ('made', 'VBD'), ('her', 'PRP$'), ('feel', 'JJ'), ('very', 'RB'), ('sleepy', 'JJ'), ('and', 'CC'), ('stupid', 'JJ'), (')', ')'), ('whether', 'IN'), ('the', 'DT'), ('pleasure', 'NN'), ('of', 'IN'), ('making', 'VBG'), ('a', 'DT'), ('daisy-chain', 'NN'), ('would', 'MD'), ('be', 'VB'), ('worth', 'IN'), ('the', 'DT'), ('trouble', 'NN'), ('of', 'IN'), ('getting', 'VBG'), ('up', 'RP'), ('and', 'CC'), ('picking', 'VBG'), ('the', 'DT'), ('daisies', 'NNS'), (',', ','), ('when', 'WRB'), ('suddenly', 'RB'), ('a', 'DT'), ('White', 'NNP'), ('Rabbit', 'NN'), ('with', 'IN'), ('pink', 'JJ'), ('eyes', 'NNS'), ('ran', 'VBD'), ('close', 'RB'), ('by', 'IN'), ('her', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"After such a fall as this, I shall think nothing of tumbling down stairs! \"\n",
    "pos_tagged = pos_tag(word_tokenize(text_alice))\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Mark', 'NNP')]), ('studied', 'VBD'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')])])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Mark studied at Stanford University\"\n",
    "#print(sentence)\n",
    "ne= ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Words to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second.',\n",
    "          'And this is the third document.',\n",
    "          'Is this first or the fourth document?']\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column headers are the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'fourth', 'is', 'or', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-term-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>or</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fourth  is  or  second  the  third  this\n",
       "0    0         1      1       0   1   0       0    1      0     1\n",
       "1    0         2      0       0   1   0       1    1      0     1\n",
       "2    1         1      0       0   1   0       0    1      1     1\n",
       "3    0         1      1       1   1   1       0    1      0     1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(),columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>or</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.386294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first    fourth   is        or    second  the  \\\n",
       "0  0.000000       1.0  1.693147  0.000000  1.0  0.000000  0.000000  1.0   \n",
       "1  0.000000       1.0  0.000000  0.000000  1.0  0.000000  2.386294  1.0   \n",
       "2  2.386294       1.0  0.000000  0.000000  1.0  0.000000  0.000000  1.0   \n",
       "3  0.000000       1.0  1.693147  2.386294  1.0  2.386294  0.000000  1.0   \n",
       "\n",
       "      third  this  \n",
       "0  0.000000   1.0  \n",
       "1  0.000000   1.0  \n",
       "2  2.386294   1.0  \n",
       "3  0.000000   1.0  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(norm=None,smooth_idf=False) # ,stop_words='english' ngram_range=(1,1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "pd.DataFrame(X.toarray(),columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Tf-Idf (words \"document\" and \"four\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "# document\n",
    "tf = 1\n",
    "idf = log(4 / 4) + 1\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.386294361119891"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "# four\n",
    "tf = 1\n",
    "idf = np.log(4 / 1) + 1\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Methods\n",
    "\n",
    "[Vader Sentiment Lexicon](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.3825, 'neg': 0.07, 'neu': 0.848, 'pos': 0.083}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = vader.SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(text_alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.8675, 'neg': 0.066, 'neu': 0.801, 'pos': 0.134}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores(text_alice) # + ':) :-) :P'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = ['i am so happy today',\n",
    "         'life is good and pleasant',\n",
    "         'i want to die because i am sad',\n",
    "         'i do not want to die life is a blessing',\n",
    "         'the world is ugly the climate is bad',\n",
    "         'there is no future for people like me']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [1,1,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>bad</th>\n",
       "      <th>because</th>\n",
       "      <th>bless</th>\n",
       "      <th>climate</th>\n",
       "      <th>die</th>\n",
       "      <th>do</th>\n",
       "      <th>for</th>\n",
       "      <th>future</th>\n",
       "      <th>...</th>\n",
       "      <th>pleasant</th>\n",
       "      <th>sad</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>to</th>\n",
       "      <th>today</th>\n",
       "      <th>ugly</th>\n",
       "      <th>want</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   am  and  bad  because  bless  climate  die  do  for  future  ...    \\\n",
       "0   1    0    0        0      0        0    0   0    0       0  ...     \n",
       "1   0    1    0        0      0        0    0   0    0       0  ...     \n",
       "2   1    0    0        1      0        0    1   0    0       0  ...     \n",
       "3   0    0    0        0      1        0    1   1    0       0  ...     \n",
       "4   0    0    1        0      0        1    0   0    0       0  ...     \n",
       "5   0    0    0        0      0        0    0   0    1       1  ...     \n",
       "\n",
       "   pleasant  sad  so  the  there  to  today  ugly  want  world  \n",
       "0         0    0   1    0      0   0      1     0     0      0  \n",
       "1         1    0   0    0      0   0      0     0     0      0  \n",
       "2         0    1   0    0      0   1      0     0     1      0  \n",
       "3         0    0   0    0      0   1      0     0     1      0  \n",
       "4         0    0   0    2      0   0      0     1     0      1  \n",
       "5         0    0   0    0      1   0      0     0     0      0  \n",
       "\n",
       "[6 rows x 29 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()#norm=None,smooth_idf=False,ngram_range=(1,1)) # ,stop_words='english' ngram_range=(1,1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(X.toarray(),columns=features)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the emotion of new examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences = ['white rabbit is happy','alice is sad']\n",
    "X_new = vectorizer.transform(new_sentences)\n",
    "classifier.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>happy</th>\n",
       "      <th>sad</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is  happy  sad  label\n",
       "0   0      1    0      1\n",
       "1   1      0    0      1\n",
       "2   0      0    1      0\n",
       "3   1      0    0      1\n",
       "4   1      0    0      0\n",
       "5   1      0    0      0"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = y\n",
    "df[df >= 1] = 1\n",
    "df[['is','happy','sad','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5848765432098773e-12 2.610725308641976e-10\n",
      "0.009803921568627453 0.9901960784313726\n"
     ]
    }
   ],
   "source": [
    "# white, rabbit, is, happy\n",
    "#pos = (0.001/6)*(0.001/6)*(0.001/6)*(2.001/6)*(1.001/6)\n",
    "#neg = (1.001/6)*(0.001/6)*(0.001/6)*(1.001/6)*(0.001/6)\n",
    "# alice, is, sad\n",
    "pos=(0.01/6)*(2.01/6)*(0.01/6)*(0.01/6)*(0.01/6)\n",
    "neg=(0.01/6)*(2.01/6)*(0.01/6)*(1.01/6)*(0.01/6)\n",
    "print(pos,neg)\n",
    "prob_pos = pos / (pos + neg)\n",
    "prob_neg = neg / (pos + neg)\n",
    "print(prob_pos,prob_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = ['i am so happy because i did not die',\n",
    "        'yesterday i was sad now i am delighted by your presence',\n",
    "        'i wish for the apocalyps',\n",
    "       'because of global warming there is no future']\n",
    "\n",
    "y_test = [1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(corpus_test)\n",
    "predictions = classifier.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
